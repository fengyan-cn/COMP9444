{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = 'e:\\\\COMP\\\\COMP9444\\\\project\\\\dumpsite_data\\\\VOC2012'\n",
    "train_path = os.path.join(root_path, \"train\")\n",
    "test_path = os.path.join(root_path, \"test\")\n",
    "file_Annotations = os.path.join(train_path, \"Annotations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'domestic garbage', 'disposed garbage', 'agriculture forestry', 'industry waste', 'construction waste', 'mining waste'}\n"
     ]
    }
   ],
   "source": [
    "object_class = []\n",
    "for each_xml in os.listdir(file_Annotations):\n",
    "    pic_xml = os.path.join(file_Annotations, each_xml)\n",
    "    tree = ET.parse(pic_xml)\n",
    "    root = tree.getroot()\n",
    "    for object_elem in root.findall('object'):\n",
    "        name_elem_value = object_elem.find('name').text\n",
    "        object_class.append(name_elem_value)\n",
    "print(set(object_class))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Due to the problem of severe sample imbalance in the dumpsite dataset (Fig. 1a), we propose two training strategies, data augmentation (vertical flipping, horizontal\n",
    "##### flipping, forward 90° rotation and reverse 90° rotation) and category balancing, to ensure the model’sefficiency during the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class  VOCDataset(Dataset):\n",
    "    def __init__(self, root, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        txt_file = os.path.join(root, \"train.txt\")\n",
    "        with open(txt_file, 'r') as f:\n",
    "            self.image_ids = f.read().strip().split()\n",
    "        self.image_folder = os.path.join(root, \"JPEGImages\")\n",
    "        self.ann_folder = os.path.join(root, \"Annotations\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_folder)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.image_ids[idx]\n",
    "        img_path = os.path.join(self.image_folder, f\"{img_id}.jpg\")\n",
    "        ann_path = os.path.join(self.ann_folder, f\"{img_id}.xml\")\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        tree = ET.parse(ann_path)\n",
    "        root = tree.getroot()\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for obj in root.findall(\"object\"):\n",
    "            label = obj.find(\"name\").text\n",
    "            print(\"label是:\", label)\n",
    "            labels.append(label)\n",
    "            bbox = obj.find(\"bndbox\")\n",
    "            xmin = int(bbox.find(\"xmin\").text)\n",
    "            ymin = int(bbox.find(\"ymin\").text)\n",
    "            xmax = int(bbox.find(\"xmax\").text)\n",
    "            ymax = int(bbox.find(\"ymax\").text)\n",
    "            if self.transforms:\n",
    "                transform = self.transforms.normalize.transforms\n",
    "                print(transform)\n",
    "                if isinstance(transform, T.Resize):\n",
    "                    print(\"等比例缩放\")\n",
    "                    scale_factor = transform.size[0] / img.width\n",
    "                    print(scale_factor)\n",
    "                    xmin = int(xmin * scale_factor)\n",
    "                    ymin = int(ymin * scale_factor)\n",
    "                    xmax = int(xmax * scale_factor)\n",
    "                    ymax = int(ymax * scale_factor)\n",
    "            box = [xmin, ymin, xmax, ymax]\n",
    "            boxes.append(box)\n",
    "        \n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "        target = {\"boxes\":boxes, \"labels\":labels}\n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torchvision.transforms import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetectionTransforms:\n",
    "    def __init__(self):\n",
    "        self.augment = T.Compose([\n",
    "            T.RandomVerticalFlip(p=0.5),\n",
    "            T.RandomHorizontalFlip(p=0.5), \n",
    "            T.RandomRotation(degrees=(0, 90), expand=False, center=None),\n",
    "            T.RandomRotation(degrees=(-90, 0), expand=False, center=None), \n",
    "        ])\n",
    "        self.normalize = T.Compose([\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    def __call__(self, image):\n",
    "        # image = self.augment(image)\n",
    "        image = self.normalize(image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "def unnormalize(image, mean, std):\n",
    "    mean = torch.tensor(mean).view(3, 1, 1)\n",
    "    std = torch.tensor(std).view(3, 1, 1)\n",
    "    return image * std * mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label是: domestic garbage\n",
      "[ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
      "label是: domestic garbage\n",
      "[ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
      "label是: agriculture forestry\n",
      "[ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
      "label是: agriculture forestry\n",
      "[ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
      "label是: domestic garbage\n",
      "[ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
      "label是: domestic garbage\n",
      "[ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
      "label是: construction waste\n",
      "[ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
      "label是: agriculture forestry\n",
      "[ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
      "label是: agriculture forestry\n",
      "[ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
      "label是: construction waste\n",
      "[ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
      "label是: domestic garbage\n",
      "[ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
      "label是: domestic garbage\n",
      "[ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
      "label是: domestic garbage\n",
      "[ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
      "label是: domestic garbage\n",
      "[ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
      "label是: domestic garbage\n",
      "[ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
      "label是: construction waste\n",
      "[ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
      "label是: domestic garbage\n",
      "[ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
      "tensor([[362., 477., 412., 532.]])\n",
      "['domestic garbage']\n",
      "label是: domestic garbage\n",
      "[ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
      "label是: construction waste\n",
      "[ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
      "label是: domestic garbage\n",
      "[ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
      "label是: construction waste\n",
      "[ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
      "label是: domestic garbage\n",
      "[ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
      "label是: construction waste\n",
      "[ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
      "label是: domestic garbage\n",
      "[ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
      "label是: industry waste\n",
      "[ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
      "label是: domestic garbage\n",
      "[ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
      "label是: construction waste\n",
      "[ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
      "label是: domestic garbage\n",
      "[ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
      "label是: agriculture forestry\n",
      "[ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
      "label是: domestic garbage\n",
      "[ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
      "label是: construction waste\n",
      "[ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
      "label是: domestic garbage\n",
      "[ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
      "label是: construction waste\n",
      "[ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
      "label是: construction waste\n",
      "[ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
      "tensor([[135., 212., 195., 268.]])\n",
      "['domestic garbage']\n",
      "label是: construction waste\n",
      "[ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
      "label是: domestic garbage\n",
      "[ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
      "label是: domestic garbage\n",
      "[ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
      "label是: agriculture forestry\n",
      "[ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
      "label是: domestic garbage\n",
      "[ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
      "label是: construction waste\n",
      "[ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
      "label是: construction waste\n",
      "[ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
      "label是: domestic garbage\n",
      "[ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
      "label是: construction waste\n",
      "[ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
      "label是: domestic garbage\n",
      "[ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
      "tensor([[406., 358., 473., 478.]])\n",
      "['construction waste']\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images = []\n",
    "    targets = []\n",
    "    for obj in batch:\n",
    "        images.append(obj[0])\n",
    "        targets.append(obj[1])\n",
    "    images = torch.stack(images, dim=0)\n",
    "    return images, targets\n",
    "\n",
    "\n",
    "transforms = DetectionTransforms()\n",
    "dataset = VOCDataset(root='./dumpsite_data/VOC2012/train', transforms=transforms)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "for images, targets in dataloader:\n",
    "    PIL_image = unnormalize(images[0], mean, std)\n",
    "    PIL_image = F.to_pil_image(PIL_image)\n",
    "    PIL_image.show()\n",
    "    print(targets[0]['boxes'])\n",
    "    print(targets[0]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[RandomHorizontalFlip(p=0.5),\n",
       " ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2), saturation=(0.8, 1.2), hue=(-0.1, 0.1)),\n",
       " RandomRotation(degrees=[-30.0, 30.0], interpolation=nearest, expand=False, fill=0),\n",
       " RandomResizedCrop(size=(300, 300), scale=(0.0, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear, antialias=True)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transforms.augment.transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024, 1024)\n"
     ]
    }
   ],
   "source": [
    "test = Image.open(\"E:/COMP/COMP9444/project/dumpsite_data/VOC2012/train/JPEGImages/CS7500.jpg\")\n",
    "print(test.size)\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label是: domestic garbage\n",
      "Resize(size=(448, 448), interpolation=bilinear, max_size=None, antialias=True)\n",
      "等比例缩放\n",
      "0.4375\n",
      "(448, 448)\n"
     ]
    }
   ],
   "source": [
    "transforms = T.Compose([\n",
    "    T.transforms.Resize((448, 448)),\n",
    "    T.transforms.ToTensor()\n",
    "])\n",
    "dataset = VOCDataset(root='./dumpsite_data/VOC2012/train', transforms=transforms)\n",
    "random_idx = random.randint(0, len(dataset) - 1)\n",
    "img, target = dataset[random_idx]\n",
    "image = F.to_pil_image(img)\n",
    "draw = ImageDraw.Draw(image)\n",
    "boxes = target['boxes']\n",
    "labels = target['labels']\n",
    "for box, label in zip(boxes, labels):\n",
    "    xmin, ymin, xmax, ymax = box\n",
    "    draw.rectangle([xmin, ymin, xmax, ymax], outline=\"red\")\n",
    "    draw.text((xmin - 10, ymin - 10), label, fill=\"red\")\n",
    "print(image.size)\n",
    "image.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_comp9444",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
