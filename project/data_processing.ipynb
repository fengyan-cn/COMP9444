{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = 'e:\\\\COMP\\\\COMP9444\\\\project\\\\dumpsite_data\\\\VOC2012'\n",
    "train_path = os.path.join(root_path, \"train\")\n",
    "test_path = os.path.join(root_path, \"test\")\n",
    "file_Annotations = os.path.join(train_path, \"Annotations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'domestic garbage', 'mining waste', 'agriculture forestry', 'construction waste', 'industry waste', 'disposed garbage'}\n"
     ]
    }
   ],
   "source": [
    "object_class = []\n",
    "for each_xml in os.listdir(file_Annotations):\n",
    "    pic_xml = os.path.join(file_Annotations, each_xml)\n",
    "    tree = ET.parse(pic_xml)\n",
    "    root = tree.getroot()\n",
    "    for object_elem in root.findall('object'):\n",
    "        name_elem_value = object_elem.find('name').text\n",
    "        object_class.append(name_elem_value)\n",
    "print(set(object_class))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Due to the problem of severe sample imbalance in the dumpsite dataset (Fig. 1a), we propose two training strategies, data augmentation (vertical flipping, horizontal\n",
    "##### flipping, forward 90° rotation and reverse 90° rotation) and category balancing, to ensure the model’sefficiency during the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class  VOCDataset(Dataset):\n",
    "    def __init__(self, root, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        txt_file = os.path.join(root, \"train.txt\")\n",
    "        with open(txt_file, 'r') as f:\n",
    "            self.image_ids = f.read().strip().split()\n",
    "        self.image_folder = os.path.join(root, \"JPEGImages\")\n",
    "        self.ann_folder = os.path.join(root, \"Annotations\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_folder)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.image_ids[idx]\n",
    "        img_path = os.path.join(self.image_folder, f\"{img_id}.jpg\")\n",
    "        ann_path = os.path.join(self.ann_folder, f\"{img_id}.xml\")\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        tree = ET.parse(ann_path)\n",
    "        root = tree.getroot()\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for obj in root.findall(\"object\"):\n",
    "            label = obj.find(\"name\").text\n",
    "            labels.append(label)\n",
    "            bbox = obj.find(\"bndbox\")\n",
    "            box = [\n",
    "                int(bbox.find(\"xmin\").text),\n",
    "                int(bbox.find(\"ymin\").text),\n",
    "                int(bbox.find(\"xmax\").text),\n",
    "                int(bbox.find(\"ymax\").text),\n",
    "            ]\n",
    "            boxes.append(box)\n",
    "        \n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "        # labels = torch.tensor(labels, dtype=torch.int64)\n",
    "        target = {\"boxes\":boxes, \"labels\":labels}\n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torchvision.transforms import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetectionTransforms:\n",
    "    def __init__(self):\n",
    "        self.augment = T.Compose([\n",
    "            # T.RandomHorizontalFlip(),\n",
    "            # T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "            T.RandomRotation(30),\n",
    "            T.RandomResizedCrop(size=(300, 300), scale=(0.0, 1.0)),\n",
    "        ])\n",
    "        self.normalize = T.Compose([\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    def __call__(self, image):\n",
    "        image = self.augment(image)\n",
    "        image = self.normalize(image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "def unnormalize(image, mean, std):\n",
    "    mean = torch.tensor(mean).view(3, 1, 1)\n",
    "    std = torch.tensor(std).view(3, 1, 1)\n",
    "    return image * std * mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[124., 667., 217., 783.],\n",
      "        [256., 613., 347., 666.]])\n",
      "['construction waste', 'construction waste']\n",
      "tensor([[182., 614., 268., 728.]])\n",
      "['domestic garbage']\n",
      "tensor([[796., 605., 916., 689.]])\n",
      "['construction waste']\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images = []\n",
    "    targets = []\n",
    "    for obj in batch:\n",
    "        images.append(obj[0])\n",
    "        targets.append(obj[1])\n",
    "    images = torch.stack(images, dim=0)\n",
    "    return images, targets\n",
    "\n",
    "\n",
    "transforms = DetectionTransforms()\n",
    "dataset = VOCDataset(root='./dumpsite_data/VOC2012/train', transforms=transforms)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "for images, targets in dataloader:\n",
    "    PIL_image = unnormalize(images[0], mean, std)\n",
    "    PIL_image = F.to_pil_image(PIL_image)\n",
    "    PIL_image.show()\n",
    "    print(targets[0]['boxes'])\n",
    "    print(targets[0]['labels'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_comp9444",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
